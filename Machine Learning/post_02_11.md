![intro](./img/intro.png)

# Hypothesis
## Hypothesis
  Hypothesis =  WX(W : Weight / W Dimension : X features * number of y classes, X : Input / X Dimension : Number of Input * X features)
  * Tensorflow에서 구현시 : hypothesis = tf.matmul(X, W)로 XW의 위치 바뀜

# Activation Function
## Softmax
  softmax는 input에 대한 출력은 0~1 사이의 값으로 정규화하며 output 값의 합이 항상 1이 되는 특성을 가짐
  * 상기 Hypothesis로 도출된 값은 Score에 불과하므로 softmax 통해서 Probabilities로 변환
  * Tensorflow에서 구현시 : hypothesis = tf.nn.softmax(hypothessis)
  
## Sigmoid

# Cost Function
  cross entropy loss(cost)는 Activation Function을 통해 도출한 Prediction value와 Ground Truth와의 차이 즉, cost를 산출
  * Tensorflow에서 구현시 : cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1)

# Gradient Descent
  Cost Function을 통해 cost를 산출했다면, cost를 낮추기 위해 W와 b, Learning Latio, Lambda 등 Hyperparameter의 Tuning이 필요 그에 대한 방법론이 Gradient Descent
  * Tensorflow에서 구현시 : optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)



* 보충 필요

![sigmoidvsReLU](./img/sigmoidvsReLU.png)

## ReLU
  * NN에서 Activation Function으로 Sigmoid를 사용할 경우 출력이 0~1이므로, Layer가 커질수록 0에 매우 근접하게 됨.
  (즉, Layer1 - Output 단에 이르게 되면 사실상 영향력이 거의 없게 됨). 이로 인해서 ReLU를 사용함.
  
## RBM(Restricted Boltzmann Machine)
  * Weights에 적절한 초기값을 주기 위함. 자세한 내용은 참고
  그러나 굳이 RBM을 사용하지 않고 적당한 initial 값을 주어도 잘 작동함
  
  
# Overfitting

## More Training data

## ~~Reducing number of features~~

## Regularization












출처 : https://gombru.github.io/2018/05/23/cross_entropy_loss/
      http://hunkim.github.io/ml/

