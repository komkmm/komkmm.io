![intro](./img/intro.png)

# Pre-processing
## Normalization
> Original data : X
> zero-centered data : X -= np.mean(X, axis = 0)
> normalized data : X /= np.std(X, axis = 0)
* image의 경우, 각 차원 간 scale이 어느 정도 맞춰져 있기 때문에 zero-centered 정도만 해주고 scale이 다양한 다른 ML 문제의 경우는 normalized 해줌
* PCA, Whitening 등의 방법도 있으나, image data에는 잘 사용하지 않음
* Train data에서 pre-processing을 했으면, Test data에도 동일하게 해줘야 함

### Batch Normalization(pre-processing은 아님)
> layer를 거치면서 data의 분포 변화가 심해져서 학습이 어려워짐. 강제로 unit gaussian distribution으로 조정. layer -> Batch Normalization -> Activation 순으로 조정
* Improves gradient flow through the network 
* Allows higher learning rates
* Reduces the strong dependence on initialization
* Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe

* Q) 강제로 gaussian으로 조정하면 원래의 구조를 잃는 것이 아닌가?
   A) 아님. 연산을 쉽게 하기 위해서 스케일 조정만 하는 것임. 자세한 것은 논문에서 확인 
* Q) shift와 scale 요소를 추가시켜 학습시키면 Identity mapping이 되어 BN이 사라지는 것 아닌지?
   A) shift와 scale이 일정량 변하긴 하지만 Identity mapping이 될 정도는 아니라서 BN이 사라지진 않음


# Hypothesis
## Hypothesis
> Hypothesis =  WX(W : Weight / W Dimension : X features * number of y classes, X : Input / X Dimension : Number of Input * X features)
  * Tensorflow에서 구현시 : hypothesis = tf.matmul(X, W)로 XW의 위치 바뀜

# Activation Functions
## Softmax
> ![equation](https://latex.codecogs.com/gif.latex?P_%7Bi%7D%20%3D%20%5Cfrac%7Bexp%28a_%7Bi%7D%29%7D%7B%5Csum_%7Bk%7Dexp%28a_%7Bk%7D%29%7D)
  * softmax는 input에 대한 출력은 0~1 사이의 값으로 정규화하며 output 값의 합이 항상 1이 되는 특성을 가짐
  * 상기 Hypothesis로 도출된 값은 Score에 불과하므로 softmax 통해서 Probabilities로 변환
  * Tensorflow에서 구현시 : hypothesis = tf.nn.softmax(hypothessis)
  
## Sigmoid
> ![equation](https://latex.codecogs.com/gif.latex?%5Csigma%28x%29%20%3D%20%5Cfrac%7B1%7D%7B%281%20&plus;%20e%5E%7B-x%7D%29%7D)
* Squashes numbers to range [0, 1]
* Historically popular since they have interpretation as a saturating "firing rate" of a neuron
> 2 problems:
* Saturated neurons "kill" the gradients(ex, if x = -10 or 10, all backprop gredient will be "0")
* Sigmoid outputs are not zero-centered(so, gradient update is not efficient)

## tanh
>![equation](https://latex.codecogs.com/gif.latex?%5Ctanh%28x%29%20%3D%20%5Cfrac%7B%281-%20e%5E%7B-x%7D%29%7D%7B%281%20&plus;%20e%5E%7B-x%7D%29%7D)
* Squashes numbers to range [-1, 1]
* zero centered but, still kills gradients when saturated

## ReLU(Rectified Linear Unit)
> ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20max%280%2C%20x%29)
* Does not saturate (in +region)
* Very computationally efficient
* Converges much faster than sigmoid/tanh in practice (e.g. 6x)
* Actually more biologically plausible than sigmoid
> 2 problems:
* not zero-centered
* when X < 0, gradients will be killed

### Leaky ReLU
> ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20max%280.01x%2C%20x%29)

### PReLU(Parametric  Rectifier)
> ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20max%28%5Calpha%20x%2C%20x%29)

### ELU(Exponential Linear Units)
> ![equation](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20%5Cbegin%7BBmatrix%7D%20x%20%26%20%5Cmbox%7Bif%7D%5C%20x%20%3E%200%20%5C%5C%20%5Calpha%28exp%28x%29%20-%201%29%20%26%20%5Cmbox%7Bif%7D%5C%20x%20%5Cleq%200%20%5Cend%7BBmatrix%7D)
* All benefits of ReLU
* Closer to zero mean outputs
* Negative saturation regime compared with Leaky ReLU adds some robustness to noise


# Cost Function
### MSE(Mean-Squared-Error)
> ![equation](https://latex.codecogs.com/gif.latex?L%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%7D%5E%7Bn%7D%28wx_%7Bi%7D-y_%7Bi%7D%29%5E%7B2%7D)
  
### Cross entropy loss
> ![equation](https://latex.codecogs.com/gif.latex?L%20%3D%20-%5Csum_%7Bj%7Dy_%7Bi%7D%5Ctextrm%7Blog%7Dp_%7Bj%7D%5C%5C%20%5Cquad%20y_%7Bi%7D%3A%20ground%5C%2Ctruth%5C%5Cp_%7Bj%7D%3A%20score%5C%2Cby%5C%2Cactivation%5C%2Cfunction)
  * cross entropy loss(cost)는 Activation Function을 통해 도출한 Prediction value와 Ground Truth와의 차이 즉, cost를 산출
  * Tensorflow에서 구현시 : cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1)

# Gradient Descent
> Cost Function을 통해 cost를 산출했다면, cost를 낮추기 위해 W와 b, Learning Latio, Lambda 등 Hyperparameter의 Tuning이 필요 그에 대한 방법론이 Gradient Descent
>> forward : compute result of an operation and save any intermediates needed for gradient computation in memory
>> backward : apply the chain rule to compute the gradient of the loss function with respect to the inputs
>> backpropagation : recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates
  * Tensorflow에서 구현시 : optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)



* 보충 필요

![sigmoidvsReLU](./img/sigmoidvsReLU.png)

## ReLU
  * NN에서 Activation Function으로 Sigmoid를 사용할 경우 출력이 0~1이므로, Layer가 커질수록 0에 매우 근접하게 됨.
  (즉, Layer1 - Output 단에 이르게 되면 사실상 영향력이 거의 없게 됨). 이로 인해서 ReLU를 사용함.
  
## RBM(Restricted Boltzmann Machine)
  * Weights에 적절한 초기값을 주기 위한 알고리즘. 자세한 내용은 참고
  그러나 굳이 RBM을 사용하지 않고 적당한 initial 값을 주어도 잘 작동함
  
  
# Overfitting

> More Training data

> ~~Reducing number of features~~

> Regularization


# Improving some methods performance of NN
## Dropout
  * Training할 때, Node 중 일부를 임의로 배제시키는 것. 

## Ensemble
  * 독립적으로 모델들을 학습시킨 후 combine 시키는 것

# Varication of NN
## Fast Forward(Resnet)
  * 일부 Layer를 건너 뛰어 
  
## Recurrent network(RNN)
  * 기존 NN이 Hierarchy상 상하로 진행되었다고 한다면, 상하좌우로 NN을 구성
  다음 포스팅에 상세 
  
# CNN(Convolutional Neural Network)
  * Input -> Convolution layer -> ReLU -> Pooling -> Convolution layer -> Relu -> Pooling ... -> FC -> output

  * Convolution layer에 Filter 적용시 size가 급격히 작아짐으로 인해 정보 손실이 커지게 될 수 있음. 이를 방지하기 위해
  padding을 통해서 급격한 size 축소를 막음으로써 정보 손실을 줄일 수 있음
  * Convolution layer output = (N - F) / stride + 1
  
  * Pooling할 때의 Filter는 activation map을 extract할 때 적용되는 Filter와 다름. 전자는 fixed, 후자는 runnable한 Filter임
  전자를 Filter가 아닌 Kernel로 지칭하기도 함.

출처 : https://gombru.github.io/2018/05/23/cross_entropy_loss/
http://hunkim.github.io/ml/
[cs231n](http://cs231n.github.io/)

<!--stackedit_data:
eyJoaXN0b3J5IjpbNDEyMzkyOTM4LDE3MzU4MTIyMTEsLTEwNT
g2NjUyMzgsLTg3MTU3Njc3Niw4ODQ1NjQ1NjQsMTUyMjg5NjY2
OCwyMzkyOTIyMDYsMTE5MDc0OTgwOSwtODMyNjUwODQ4LDE1MT
g3MTEwMTUsLTI5NDkzMzgxMywtMjEyODIzMTczMiwtMTg4MzU1
ODQyOCwyMDk4MzEzOTI0LDU5MzI4NzM5OSwtMjA5NzAwODEzNC
wtNTM3MzkxNTYxXX0=
-->