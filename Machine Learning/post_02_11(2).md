# NN
> A class of function(including linear function, non-linear function, etc..) stacking hierarchical simpler functions in order to make more complex  non-linear functions.
eg) 2-layer Neural Network
>>![equation](https://latex.codecogs.com/gif.latex?f%20%3D%20W_%7B2%7Dmax%280%2C%20W_%7B1%7Dx%29)


# CNN(Convolutional Neural Network)
  * Input -> Convolution layer -> ReLU -> Pooling -> Convolution layer -> Relu -> Pooling ... -> FC -> output

> Convolution layer
  * Convolution layer에 Filter 적용시 size가 급격히 작아짐으로 인해 정보 손실이 커지게 될 수 있음. 이를 방지하기 위해
  padding을 통해서 급격한 size 축소를 막음으로써 정보 손실을 줄일 수 있음
  * Convolution layer output = (N - F) / stride + 1
  
>Pooling layer
>>*makes the representations smaller and more manageable
>>*operates over each activation map independently

  * Pooling할 때의 Filter는 activation map을 extract할 때 적용되는 Filter와 다름. 전자는 fixed, 후자는 runnable한 Filter임
  전자를 Filter가 아닌 Kernel로 지칭하기도 함.
  * Pooling시에는 padding을 하지 않음. 이유는 Downsampling이 
  목적이며, corner 값이 간과되는 경우도 없기 때문
  
  * Q: Pooling시 겹치치 않게 하는 것이 일반적인지? 
     A: Yes. Downsampling이 목적이기 때문에
  * Q: Max Pooling이 Average Pooling보다 좋은 이유?
	 A: 직관적으로 어떤 signal에 대해서 얼마나 활성화 되었는지를 판단하기 위해서는 Max값을 활용하는 것이 더 유용
  * Q: Pooling보다 Stride가 동일한 것 아닌지?
     A: 실제로 Pooling 대신에 Stride를 사용하는 사람도 많다.

> Regularization
>> Train data가 overfitting 하지 않도록 하는 것

	* Dropout
	* Batch Normalization -> 일반적으로 많이 사용
	* Data Augmentation
	* DropConnect
	* Fractional Max Pooling
	* Stochastic Depth

# RNN(Recurrent Neural Network)
  * Language와 같은 Sequence data에서 NN, CNN을 통해서 처리할 수 없기 때문에 등장
  (Time step에 따라서 이전의 result가 다음 result에 영향을 미칠 수 있어야 하기 때문)
  	
   
<!--stackedit_data:
eyJoaXN0b3J5IjpbNDM2NzI4NzQ3LDIwNDYwNDQyOTEsLTEwNj
gwOTYyNjUsMTA2MTg2MDc5LDgwNzIzMjUyOSwxNjAwMzUwMTM2
LDcxOTUxMDY3NCw4NDcyMzQ0MzAsLTIxMDU1MzQ0MDEsLTgxND
EzNjgyNiwyMTI3OTMwMDM2XX0=
-->